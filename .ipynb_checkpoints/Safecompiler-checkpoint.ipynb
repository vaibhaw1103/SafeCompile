{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e156559e-c807-4889-974e-b67899c1a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "token_patterns = {\n",
    "    \"KEYWORD\": r\"\\b(int|float|double|char|void|if|else|for|while|do|switch|case|break|continue|return|struct|union|typedef|const|static|enum|sizeof)\\b\",\n",
    "    \"IDENTIFIER\": r\"\\b[A-Za-z_][A-Za-z0-9_]*\\b\",\n",
    "    \"NUMBER\": r\"\\b\\d+(\\.\\d+)?\\b\",\n",
    "    \"CHAR\": r\"'[^']'\",\n",
    "    \"STRING\": r'\"[^\"\\n]*\"',\n",
    "    \"PREPROCESSOR\": r\"#\\s*[a-zA-Z_]+\\b.*\",\n",
    "    \"OPERATOR\": r\"(\\+\\+|--|\\+=|-=|\\*=|/=|%=|==|!=|>=|<=|&&|\\|\\||<<|>>|[+\\-*/%=&|^~!<>])\",\n",
    "    \"SEPARATOR\": r\"[{}\\[\\]();,:]\",\n",
    "    \"COMMENT\": r\"(//.*?$|/\\*[\\s\\S]*?\\*/)\",\n",
    "    \"WHITESPACE\": r\"[ \\t\\n]+\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5739ea9f-b7e8-4488-bb11-b1eb193a0911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD: \\b(int|float|double|char|void|if|else|for|while|do|switch|case|break|continue|return|struct|union|typedef|const|static|enum|sizeof)\\b\n",
      "IDENTIFIER: \\b[A-Za-z_][A-Za-z0-9_]*\\b\n",
      "NUMBER: \\b\\d+(\\.\\d+)?\\b\n",
      "CHAR: '[^']'\n",
      "STRING: \"[^\"\\n]*\"\n",
      "PREPROCESSOR: #\\s*[a-zA-Z_]+\\b.*\n",
      "OPERATOR: (\\+\\+|--|\\+=|-=|\\*=|/=|%=|==|!=|>=|<=|&&|\\|\\||<<|>>|[+\\-*/%=&|^~!<>])\n",
      "SEPARATOR: [{}\\[\\]();,:]\n",
      "COMMENT: (//.*?$|/\\*[\\s\\S]*?\\*/)\n",
      "WHITESPACE: [ \\t\\n]+\n"
     ]
    }
   ],
   "source": [
    "# Print all token patterns to verify\n",
    "for token_type, pattern in token_patterns.items():\n",
    "    print(f\"{token_type}: {pattern}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beeff630-8029-4630-9d4e-c4e3b1d3b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "token_patterns = {\n",
    "    \"KEYWORD\": r\"\\b(int|float|double|char|void|if|else|for|while|do|switch|case|break|continue|return|struct|union|typedef|const|static|enum|sizeof)\\b\",\n",
    "    \"IDENTIFIER\": r\"\\b[A-Za-z_][A-Za-z0-9_]*\\b\",\n",
    "    \"NUMBER\": r\"\\b\\d+(\\.\\d+)?\\b\",\n",
    "    \"CHAR\": r\"'[^']'\",\n",
    "    \"STRING\": r'\"[^\"\\n]*\"',\n",
    "    \"PREPROCESSOR\": r\"#\\s*[a-zA-Z_]+\\b.*\",\n",
    "    \"OPERATOR\": r\"(\\+\\+|--|\\+=|-=|\\*=|/=|%=|==|!=|>=|<=|&&|\\|\\||<<|>>|[+\\-*/%=&|^~!<>])\",\n",
    "    \"SEPARATOR\": r\"[{}\\[\\]();,:]\",\n",
    "    \"COMMENT\": r\"(//.*?$|/\\*[\\s\\S]*?\\*/)\",\n",
    "    \"WHITESPACE\": r\"[ \\t\\n]+\"\n",
    "}\n",
    "\n",
    "combined_pattern = '|'.join(f\"(?P<{name}>{pattern})\" for name, pattern in token_patterns.items())\n",
    "token_regex = re.compile(combined_pattern, re.MULTILINE)\n",
    "\n",
    "def tokenize(code):\n",
    "    tokens = []\n",
    "    for match in re.finditer(token_regex, code):\n",
    "        kind = match.lastgroup\n",
    "        value = match.group()\n",
    "        if kind == \"WHITESPACE\" or kind == \"COMMENT\":\n",
    "            continue  # skip\n",
    "        tokens.append((kind, value))\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1112dd7-c446-438e-ad2f-5ed419fa5ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSOR → #include <stdio.h>\n",
      "KEYWORD      → int\n",
      "IDENTIFIER   → main\n",
      "SEPARATOR    → (\n",
      "SEPARATOR    → )\n",
      "SEPARATOR    → {\n",
      "KEYWORD      → char\n",
      "IDENTIFIER   → str\n",
      "SEPARATOR    → [\n",
      "NUMBER       → 10\n",
      "SEPARATOR    → ]\n",
      "SEPARATOR    → ;\n",
      "IDENTIFIER   → gets\n",
      "SEPARATOR    → (\n",
      "IDENTIFIER   → str\n",
      "SEPARATOR    → )\n",
      "SEPARATOR    → ;\n",
      "IDENTIFIER   → printf\n",
      "SEPARATOR    → (\n",
      "STRING       → \"Hello, %s\"\n",
      "SEPARATOR    → ,\n",
      "IDENTIFIER   → str\n",
      "SEPARATOR    → )\n",
      "SEPARATOR    → ;\n",
      "KEYWORD      → return\n",
      "NUMBER       → 0\n",
      "SEPARATOR    → ;\n",
      "SEPARATOR    → }\n"
     ]
    }
   ],
   "source": [
    "sample_code = \"\"\"\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    char str[10];\n",
    "    gets(str);\n",
    "    printf(\"Hello, %s\", str);\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "token_list = tokenize(sample_code)\n",
    "\n",
    "# Print output\n",
    "for token_type, token_value in token_list:\n",
    "    print(f\"{token_type:12} → {token_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845a3b5-e7b2-40c8-b815-e49997685175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
