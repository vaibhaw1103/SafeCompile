{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53cc3bb-cec8-42f1-8c46-999de8446fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the CodeXGLUE vulnerability detection dataset\n",
    "dataset = load_dataset(\"google/code_x_glue_cc_defect_detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a41608f-f63a-4a75-b3b9-155b37a94219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert and save\n",
    "df_train = pd.DataFrame(dataset[\"train\"])\n",
    "df_valid = pd.DataFrame(dataset[\"validation\"])\n",
    "df_test  = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "df_train.to_csv(\"devign_train.csv\", index=False)\n",
    "df_valid.to_csv(\"devign_valid.csv\", index=False)\n",
    "df_test.to_csv(\"devign_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd6dfa0-83a1-4cc2-b461-b55fe18115ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load CodeBERT tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Set model to eval mode (no training)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4b46347-0b69-4a3c-83d4-66d6195976e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to embed one C function using CodeBERT\n",
    "def embed_codebert(func_str, tokenizer, model, max_length=256):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(func_str, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Use the [CLS] token embedding as the vector representation\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return cls_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edd7cbf2-4249-42f4-a861-e404d852bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding functions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [10:34<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use a small sample to start (e.g., first 1000 for faster processing)\n",
    "sample_size = 1000\n",
    "funcs = dataset[\"train\"][\"func\"][:sample_size]\n",
    "labels = dataset[\"train\"][\"target\"][:sample_size]\n",
    "\n",
    "# Generate embeddings for each function\n",
    "embeddings = []\n",
    "for func in tqdm(funcs, desc=\"Embedding functions\"):\n",
    "    try:\n",
    "        vec = embed_codebert(func, tokenizer, model)\n",
    "        embeddings.append(vec)\n",
    "    except Exception as e:\n",
    "        print(\"Error embedding function:\", e)\n",
    "        embeddings.append(np.zeros(768))  # fallback if error\n",
    "\n",
    "# Convert to array\n",
    "X = np.vstack(embeddings)\n",
    "y = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f47d0a8-a8f6-471f-9843-10e979d7d32f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Split data for internal validation\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train Logistic Regression\u001b[39;00m\n\u001b[0;32m      9\u001b[0m clf \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split data for internal validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5ee52-b7ed-4400-bb0e-eaf7c80cdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_rf_pred = rf_clf.predict(X_val)\n",
    "print(\"üìä Random Forest Evaluation:\\n\")\n",
    "print(classification_report(y_val, y_rf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a2741-8b35-4e7c-a87f-418d8996b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_functions_batched(functions, tokenizer, model, batch_size=32, max_len=256):\n",
    "    \"\"\"\n",
    "    Embed a list of C/C++ functions using CodeBERT in batches.\n",
    "    Returns: np.ndarray of shape (n_samples, 768)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(functions), batch_size):\n",
    "        batch = functions[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True,\n",
    "                           truncation=True, max_length=max_len)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "        # Extract [CLS] embeddings\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.append(cls_embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3dbc0e-4875-49c8-9e27-a98fe6a18aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create and train the model\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_xgb_pred = xgb_clf.predict(X_val)\n",
    "\n",
    "# Show results\n",
    "print(\"üìä XGBoost Evaluation:\\n\")\n",
    "print(classification_report(y_val, y_xgb_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63977cc6-c37c-4348-875b-1d070370d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full training data from Hugging Face dataset\n",
    "funcs_train_full = dataset[\"train\"][\"func\"]\n",
    "labels_train_full = dataset[\"train\"][\"target\"]\n",
    "\n",
    "print(\"Total training functions:\", len(funcs_train_full))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f020b6db-431b-41ef-9075-446f69916cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Block 1\n",
      "from datasets import load_dataset\n",
      "\n",
      "# Load the CodeXGLUE vulnerability detection dataset\n",
      "dataset = load_dataset(\"google/code_x_glue_cc_defect_detection\")\n",
      "\n",
      "\n",
      "# Block 2\n",
      "import pandas as pd\n",
      "\n",
      "# Convert and save\n",
      "df_train = pd.DataFrame(dataset[\"train\"])\n",
      "df_valid = pd.DataFrame(dataset[\"validation\"])\n",
      "df_test  = pd.DataFrame(dataset[\"test\"])\n",
      "\n",
      "df_train.to_csv(\"devign_train.csv\", index=False)\n",
      "df_valid.to_csv(\"devign_valid.csv\", index=False)\n",
      "df_test.to_csv(\"devign_test.csv\", index=False)\n",
      "\n",
      "\n",
      "# Block 3\n",
      "from transformers import RobertaTokenizer, RobertaModel\n",
      "import torch\n",
      "\n",
      "# Load CodeBERT tokenizer and model\n",
      "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
      "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
      "\n",
      "# Set model to eval mode (no training)\n",
      "model.eval()\n",
      "\n",
      "\n",
      "# Block 4\n",
      "# Function to embed one C function using CodeBERT\n",
      "def embed_codebert(func_str, tokenizer, model, max_length=256):\n",
      "    # Tokenize\n",
      "    inputs = tokenizer(func_str, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        outputs = model(**inputs)\n",
      "\n",
      "    # Use the [CLS] token embedding as the vector representation\n",
      "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
      "    return cls_embedding\n",
      "\n",
      "\n",
      "# Block 5\n",
      "import numpy as np\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Use a small sample to start (e.g., first 1000 for faster processing)\n",
      "sample_size = 1000\n",
      "funcs = dataset[\"train\"][\"func\"][:sample_size]\n",
      "labels = dataset[\"train\"][\"target\"][:sample_size]\n",
      "\n",
      "# Generate embeddings for each function\n",
      "embeddings = []\n",
      "for func in tqdm(funcs, desc=\"Embedding functions\"):\n",
      "    try:\n",
      "        vec = embed_codebert(func, tokenizer, model)\n",
      "        embeddings.append(vec)\n",
      "    except Exception as e:\n",
      "        print(\"Error embedding function:\", e)\n",
      "        embeddings.append(np.zeros(768))  # fallback if error\n",
      "\n",
      "# Convert to array\n",
      "X = np.vstack(embeddings)\n",
      "y = np.array(labels)\n",
      "\n",
      "\n",
      "# Block 6\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# Split data for internal validation\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Train Logistic Regression\n",
      "clf = LogisticRegression(max_iter=1000)\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict and evaluate\n",
      "y_pred = clf.predict(X_val)\n",
      "print(classification_report(y_val, y_pred))\n",
      "\n",
      "\n",
      "# Block 7\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# Train Random Forest\n",
      "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "rf_clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict and evaluate\n",
      "y_rf_pred = rf_clf.predict(X_val)\n",
      "print(\"üìä Random Forest Evaluation:\\n\")\n",
      "print(classification_report(y_val, y_rf_pred))\n",
      "\n",
      "\n",
      "# Block 8\n",
      "def embed_functions_batched(functions, tokenizer, model, batch_size=32, max_len=256):\n",
      "    \"\"\"\n",
      "    Embed a list of C/C++ functions using CodeBERT in batches.\n",
      "    Returns: np.ndarray of shape (n_samples, 768)\n",
      "    \"\"\"\n",
      "    model.eval()\n",
      "    all_embeddings = []\n",
      "\n",
      "    for i in range(0, len(functions), batch_size):\n",
      "        batch = functions[i:i + batch_size]\n",
      "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True,\n",
      "                           truncation=True, max_length=max_len)\n",
      "        with torch.no_grad():\n",
      "            \n",
      "            outputs = model(**inputs)\n",
      "        # Extract [CLS] embeddings\n",
      "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
      "        all_embeddings.append(cls_embeddings)\n",
      "\n",
      "    return np.vstack(all_embeddings)\n",
      "\n",
      "\n",
      "# Block 9\n",
      "import xgboost as xgb\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# Create and train the model\n",
      "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
      "xgb_clf.fit(X_train, y_train)\n",
      "\n",
      "# Predict on validation set\n",
      "y_xgb_pred = xgb_clf.predict(X_val)\n",
      "\n",
      "# Show results\n",
      "print(\"üìä XGBoost Evaluation:\\n\")\n",
      "print(classification_report(y_val, y_xgb_pred))\n",
      "\n",
      "\n",
      "\n",
      "# Block 10\n",
      "# Load full training data from Hugging Face dataset\n",
      "funcs_train_full = dataset[\"train\"][\"func\"]\n",
      "labels_train_full = dataset[\"train\"][\"target\"]\n",
      "\n",
      "print(\"Total training functions:\", len(funcs_train_full))\n",
      "\n",
      "\n",
      "# Block 11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Replace this with your actual notebook filename\n",
    "notebook_path = \"model.ipynb\"\n",
    "\n",
    "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    notebook = json.load(f)\n",
    "\n",
    "code_blocks = []\n",
    "for cell in notebook[\"cells\"]:\n",
    "    if cell[\"cell_type\"] == \"code\":\n",
    "        code = \"\".join(cell[\"source\"])\n",
    "        code_blocks.append(code)\n",
    "\n",
    "# Print all code blocks\n",
    "for i, block in enumerate(code_blocks):\n",
    "    print(f\"# Block {i+1}\\n{block}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd65fa20-00e6-49a7-b3ae-768f434be755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Batched embedding of full training functions\n",
    "funcs_train_full = dataset[\"train\"][\"func\"]\n",
    "labels_train_full = dataset[\"train\"][\"target\"]\n",
    "\n",
    "print(f\"Total training functions: {len(funcs_train_full)}\")\n",
    "\n",
    "embeddings_full = embed_functions_batched(funcs_train_full, tokenizer, model, batch_size=32, max_len=256)\n",
    "print(f\"Embeddings shape: {embeddings_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f0cfe-2434-4a41-820a-42fd73227402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def embed_functions_batched(functions, tokenizer, model, batch_size=16, max_len=256):\n",
    "    \"\"\"\n",
    "    Embed a list of C/C++ functions using CodeBERT in batches.\n",
    "    Returns: np.ndarray of shape (n_samples, 768)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(functions), batch_size):\n",
    "        batch = functions[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True,\n",
    "                           truncation=True, max_length=max_len).to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.append(cls_embeddings)\n",
    "\n",
    "        # Clear cache after each batch to avoid OOM\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.vstack(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbdcd1b8-0b34-47b5-bd07-a3e3f1656a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2732/2732 [55:52<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding complete. Shape: (21854, 768)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Make sure model is loaded and moved to device\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def embed_functions_batched_safe(functions, tokenizer, model, batch_size=8, max_len=256):\n",
    "    \"\"\"\n",
    "    Embed a list of C/C++ functions using CodeBERT in batches.\n",
    "    Returns: np.ndarray of shape (n_samples, 768)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(functions), batch_size), desc=\"Embedding batches\"):\n",
    "        batch = functions[i:i + batch_size]\n",
    "\n",
    "        try:\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True,\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embeddings.append(cls_embeddings)\n",
    "\n",
    "            # Optional: clear cache if you face memory issues\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at batch starting index {i}: {e}\")\n",
    "            all_embeddings.append(np.zeros((len(batch), 768)))\n",
    "\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Usage:\n",
    "funcs_train_full = dataset[\"train\"][\"func\"]\n",
    "\n",
    "embeddings_full = embed_functions_batched_safe(funcs_train_full, tokenizer, model, batch_size=8)\n",
    "\n",
    "print(f\"Embedding complete. Shape: {embeddings_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3414a09b-a8dd-46d0-9281-f4a99fda963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded embeddings and labels from disk.\n",
      "Embeddings shape: (21854, 768)\n",
      "Labels shape: (21854,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# üîÅ Load previously saved embeddings\n",
    "embeddings_full = np.load(\"codebert_embeddings.npy\")\n",
    "\n",
    "# üîÅ Load previously saved labels\n",
    "labels_train_full = pd.read_csv(\"codebert_labels.csv\")[\"label\"].values\n",
    "\n",
    "print(\"‚úÖ Loaded embeddings and labels from disk.\")\n",
    "print(f\"Embeddings shape: {embeddings_full.shape}\")\n",
    "print(f\"Labels shape: {labels_train_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c1ddfb-b287-406b-8b2f-7a5b8d7b00ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings: (21854, 768), labels: (21854,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load previously saved files\n",
    "X = np.load(\"codebert_embeddings.npy\")\n",
    "y = pd.read_csv(\"codebert_labels.csv\")[\"label\"].values\n",
    "\n",
    "print(f\"Loaded embeddings: {X.shape}, labels: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f1f68b-adac-4681-95a4-75e1cfdfb5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance on Full Dataset:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.74      0.67      2367\n",
      "        True       0.59      0.44      0.50      2004\n",
      "\n",
      "    accuracy                           0.60      4371\n",
      "   macro avg       0.60      0.59      0.58      4371\n",
      "weighted avg       0.60      0.60      0.59      4371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split full data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    embeddings_full, labels_train_full, test_size=0.2, random_state=42, stratify=labels_train_full)\n",
    "\n",
    "# Initialize and train Logistic Regression\n",
    "clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on validation set\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"Logistic Regression Performance on Full Dataset:\\n\")\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7271c64-33ca-4f31-aff3-bd1f51aeb971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance on Full Dataset:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.59      0.74      0.66      2367\n",
      "        True       0.56      0.40      0.47      2004\n",
      "\n",
      "    accuracy                           0.58      4371\n",
      "   macro avg       0.58      0.57      0.56      4371\n",
      "weighted avg       0.58      0.58      0.57      4371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on validation set\n",
    "y_rf_pred = rf_clf.predict(X_val)\n",
    "print(\"Random Forest Performance on Full Dataset:\\n\")\n",
    "print(classification_report(y_val, y_rf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bda38cda-e74b-4f99-b93e-e785fd3dadce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä XGBoost Performance on Full Dataset:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.59      0.65      0.62      2367\n",
      "        True       0.53      0.48      0.50      2004\n",
      "\n",
      "    accuracy                           0.57      4371\n",
      "   macro avg       0.56      0.56      0.56      4371\n",
      "weighted avg       0.57      0.57      0.57      4371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_xgb_pred = xgb_clf.predict(X_val)\n",
    "\n",
    "print(\"üìä XGBoost Performance on Full Dataset:\\n\")\n",
    "print(classification_report(y_val, y_xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccef734a-34ea-4280-8afd-1c0f075de6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved as vuln_xgb_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save XGBoost classifier\n",
    "joblib.dump(xgb_clf, \"vuln_xgb_model.pkl\")\n",
    "\n",
    "print(\"‚úÖ Model saved as vuln_xgb_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afaf3e92-c7c6-4bd1-988b-fb51e5cca719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluation Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.59      0.65      0.62      2367\n",
      "        True       0.53      0.48      0.50      2004\n",
      "\n",
      "    accuracy                           0.57      4371\n",
      "   macro avg       0.56      0.56      0.56      4371\n",
      "weighted avg       0.57      0.57      0.57      4371\n",
      "\n",
      "‚úÖ Accuracy: 0.5696636925188744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = xgb_clf.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "print(\"üìä Evaluation Results:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"‚úÖ Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb1d240-93ab-48e7-a277-c3d63821098f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.eval()\n",
    "\n",
    "# Load trained XGBoost model\n",
    "xgb_clf = joblib.load(\"vuln_xgb_model.pkl\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e83b3b69-a10e-4111-9873-cecacb50a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_vulnerability(code_snippet: str) -> str:\n",
    "    \"\"\"\n",
    "    Predict if a single C/C++ code snippet is vulnerable using CodeBERT + XGBoost.\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get CodeBERT [CLS] embedding\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # shape: (1, 768)\n",
    "\n",
    "    # Predict using trained XGBoost model\n",
    "    prediction = xgb_clf.predict(embedding)[0]\n",
    "    proba = xgb_clf.predict_proba(embedding)[0]\n",
    "\n",
    "    label = \"VULNERABLE\" if prediction == 1 else \"SAFE\"\n",
    "    confidence = proba[prediction]\n",
    "\n",
    "    return f\"üîç Prediction: {label} (Confidence: {confidence:.2f})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04e8679c-44aa-4bac-a1ab-0bf3a5db87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsafe_code = \"\"\"\n",
    "#include <stdio.h>\n",
    "\n",
    "void vulnerable_function() {\n",
    "    char buffer[64];\n",
    "    printf(\"Enter your name: \");\n",
    "    gets(buffer);  // ‚ùå Unsafe: allows buffer overflow\n",
    "    printf(\"Hello, %s\\\\n\", buffer);\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dede5c1c-ccb5-4a16-8dcd-69302a74c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "def predict_vulnerability(code_snippet, model, tokenizer, clf_model_path=\"vuln_xgb_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Predict if the given code snippet is vulnerable using CodeBERT and trained ML model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Tokenize and embed\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # shape (1, 768)\n",
    "\n",
    "    # Load classifier\n",
    "    clf = joblib.load(clf_model_path)\n",
    "\n",
    "    # Predict\n",
    "    pred = clf.predict(cls_embedding)[0]\n",
    "    proba = clf.predict_proba(cls_embedding)[0][pred]\n",
    "\n",
    "    label = \"VULNERABLE\" if pred == 1 else \"SAFE\"\n",
    "    print(f\"üîç Prediction: {label} (confidence: {proba:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45fed45-e4c0-401b-977e-ea2a5cef3d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
